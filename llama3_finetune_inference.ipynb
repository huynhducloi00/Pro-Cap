{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9584d026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3.743171691894531e-05\n",
      "2 0.00032806396484375\n",
      "3 0.00011682510375976562\n",
      "4 0.00010776519775390625\n"
     ]
    }
   ],
   "source": [
    "from utils import set_env\n",
    "import time\n",
    "set_env()\n",
    "import pandas as pd\n",
    "from datasets.arrow_dataset import Dataset\n",
    "import pyarrow as pa\n",
    "i1=time.time()\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "i2=time.time()\n",
    "print('1', i2-i1)\n",
    "from datasets import load_dataset\n",
    "i3=time.time()\n",
    "print('2',i3-i2)\n",
    "import torch\n",
    "i4=time.time()\n",
    "print('3', i4-i3)\n",
    "\n",
    "from trl import SFTTrainer\n",
    "i5=time.time()\n",
    "print('4',i5-i4)\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTConfig\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d34ccc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "model_original = AutoModelForCausalLM.from_pretrained(base_model_id, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True, \n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b422e214-977a-4c45-9d66-b6a698c546eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH='Data'\n",
    "DATASET='mami'\n",
    "# dataset_name = \"scooterman/guanaco-llama3-1k\"\n",
    "train, test= torch.load(f'{DATA_PATH}/{DATASET}_trainset_llm.pkl', weights_only=True), torch.load(f'{DATA_PATH}/{DATASET}_testset_llm.pkl', weights_only=True)\n",
    "for i in range(len(train)):\n",
    "    train[i]['prompt_all_text']=f\"{train[i]['prompt_all_text']} {'good' if train[i]['label']==0 else 'bad'}\"\n",
    "train=Dataset(pa.Table.from_pylist([{'text':x['prompt_all_text']} for x in train]))\n",
    "# test=Dataset(pa.Table.from_pylist([{'text':x['prompt_all_text']} for x in test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2cac79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Milk Milkzip. Context: A pitcher of milk and a piece of cheese. The person in the image is a christian. Entity: Archive milk zip software lossless compression skimmed milk powdered milk cheese. Demographic: Unk. Classification: good\n",
      "\n",
      "Example 2: Nappcom Perfect Women Starts undressing while you play video games MemeCentercom. Context: A woman in a lingerie sitting on a couch. A white woman in a lingerie posing on a couch in front of a tv screen with a . A woman is sitting on a couch in a lingerie and a t-shirt with a picture of a . Switzerland is the country that the person in the image comes from switzerland is the country that the person in the image. A woman in a lingerie posing on a couch. Entity: Breast photo shoot science geometry violet adipose tissue sternum physics surgical operation mammary gland line thorax brand light skin prosthesis mathematics screenshot font breast augmentation. Demographic: White female white female. Classification: bad\n",
      "\n",
      "Question: HOW YOU SIT IN THE CAR OUTSIDE OF YOUR HOUSE AFTER YOU WENT TO BUY SOME ESSENTIALS BUT you are NOT READY TO GO INSIDE YET made with mematic. Context: A man sitting in a car with a quote. He is a white man in a car with a quote on the side of the car. He is a man in a car with a quote on the side of the car. U s of america - u s of america - u s of america - u . He is a christian and he is driving a car with a christian quote on the side of the. Entity: Carla gugino faster george tillman jr. action actor create dwayne johnson screenwriter action thriller. Demographic: Black male black male. Classification:  good\n"
     ]
    }
   ],
   "source": [
    "print(train[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99d29de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: FACEBOOK SINGLES GROUPS BELIKE WHEN A NEW WOMAN JOINS THE GROUP imgflipcom. Context: A group of people posing for a picture. He is a white man with a blond wig and a blond wig and a blond . A man and a woman are posing in front of a mirror. Switzerland - switzerland - switzerland - switzerland - s. He is a christian, he is a muslim, he is a jew, he is. Entity: Sitcom kaley cuoco sheldon cooper bill prady johnny galecki jim parsons the big bang theory penny chuck lorre. Demographic: White male indian male white male white female white male indian male. Classification: good\n",
      "\n",
      "Example 2: Normal gamers use this Pro gamers use this But only LEGENDS use this MemeCentercom. Context: A picture of a chair and a picture of a chair and a picture of a chair and a picture of a. The person in the image is a christian agnostic agnostic agnostic agno. Entity: Design gaming chair legend gamer good create. Demographic: Unk. Classification: good\n",
      "\n",
      "Question: Access USER USER Jenner ate grilled cheese fries crepes and more before hitting the #MetGala red carpet Now that is bravery ahwdtv EzCnvf Pas I get that you are saying it like it is a big deal but I do not get why it is a big deal. Context: A picture of a group of people in a room. The person in the image is a christian and the person in the image is a muslim and the person in the image. Entity: Cartoon the metropolitan museum of art human behavior kendall jenner facial hair public relations conversation keeping up with the kardashians model behavior human hair vogue fiction 2017 met gala cat red carpet 2017 celebrity 2021 met gala. Demographic: Black male east asian female white male middle eastern female black female middle eastern female middle eastern male middle eastern male black male middle eastern male. Classification: \n"
     ]
    }
   ],
   "source": [
    "print(test[0]['prompt_all_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93b0c17-77af-4328-889a-d418d0814101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory where the results and checkpoint are stored\n",
    "import copy\n",
    "\n",
    "\n",
    "output_dir = \"./results\"\n",
    "max_grad_norm = 0.3\n",
    "learning_rate = 2e-4\n",
    "weight_decay = 0.001\n",
    "optim = \"paged_adamw_32bit\"\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 100\n",
    "logging_steps = 5\n",
    "training_arguments = SFTConfig(\n",
    "    max_seq_length=1024,\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=10,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=optim,\n",
    "    save_steps=100,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    bf16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    dataset_text_field=\"text\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "model=copy.deepcopy(model_original)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c25576fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(task='text-generation', model=model, tokenizer=tokenizer,\n",
    "    stop_strings=[\"\\n\",'.'],max_new_tokens=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "395463ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 231/995 [00:18<00:59, 12.77it/s, Acc:1.0000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m pbar\u001b[38;5;241m=\u001b[39mtrange(\u001b[38;5;28mlen\u001b[39m(test))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m----> 5\u001b[0m     res\u001b[38;5;241m=\u001b[39m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt_all_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m     res_1\u001b[38;5;241m=\u001b[39mres[\u001b[38;5;28mlen\u001b[39m(test[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_all_text\u001b[39m\u001b[38;5;124m'\u001b[39m]):]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      7\u001b[0m     res1\u001b[38;5;241m.\u001b[39mappend(res_1)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:272\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1302\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         )\n\u001b[1;32m   1300\u001b[0m     )\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1309\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1308\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1309\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1310\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1209\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1208\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1209\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:370\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    368\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 370\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1867\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1864\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39m_pad_token_tensor \u001b[38;5;241m=\u001b[39m pad_token_tensor\n\u001b[1;32m   1865\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39m_decoder_start_token_tensor \u001b[38;5;241m=\u001b[39m decoder_start_token_tensor\n\u001b[0;32m-> 1867\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\n\u001b[1;32m   1869\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1870\u001b[0m     inputs: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1871\u001b[0m     generation_config: Optional[GenerationConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1872\u001b[0m     logits_processor: Optional[LogitsProcessorList] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1873\u001b[0m     stopping_criteria: Optional[StoppingCriteriaList] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1874\u001b[0m     prefix_allowed_tokens_fn: Optional[Callable[[\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor], List[\u001b[38;5;28mint\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1875\u001b[0m     synced_gpus: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1876\u001b[0m     assistant_model: Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedModel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1877\u001b[0m     streamer: Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseStreamer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1878\u001b[0m     negative_prompt_ids: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1879\u001b[0m     negative_prompt_attention_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1880\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1881\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[GenerateOutput, torch\u001b[38;5;241m.\u001b[39mLongTensor]:\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1883\u001b[0m \n\u001b[1;32m   1884\u001b[0m \u001b[38;5;124;03m    Generates sequences of token ids for models with a language modeling head.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;124;03m                - [`~generation.GenerateBeamEncoderDecoderOutput`]\u001b[39;00m\n\u001b[1;32m   1964\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;66;03m# 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "correct=0\n",
    "res1=[]\n",
    "pbar=trange(len(test))\n",
    "for i in pbar:\n",
    "    res=pipe(test[i]['prompt_all_text'], tokenizer=tokenizer)[0]['generated_text']\n",
    "    res_1=res[len(test[i]['prompt_all_text']):].strip()\n",
    "    res1.append(res_1)\n",
    "    guess=0 if res_1=='good' else 1\n",
    "    correct+=guess==test[i]['label']\n",
    "    pbar.set_postfix_str(f\"Acc:{correct/(i+1):.4f}\")\n",
    "print(pd.Series(res1).apply(lambda x:x.strip()).value_counts())\n",
    "print(f\"Acc:{correct/len(test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56498018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=1\n",
    "res=pipe(test[i]['prompt_all_text'], tokenizer=tokenizer)[0]['generated_text']\n",
    "\n",
    "print(res[len(test[i]['prompt_all_text']):].strip())\n",
    "test[1]['label']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
